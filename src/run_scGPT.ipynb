{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# need to run on GPU \n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import json \n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id\n",
    "\n",
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GEARS\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "dataset_name = \"southard_rpe1_crispri\"   # change if needed\n",
    "test_train_config_path = \"/.mounts/labs/steinlab/scratch/mtong/datasets/seq/southard_RPE1_CRISPRi/southard_data/results/set2conditions.json\"\n",
    "patience = 1 # how many epochs with increasing error are allowed\n",
    "epochs = 15\n",
    "pool_size = 100\n",
    "seed = 1\n",
    "working_dir = \"/.mounts/labs/steinlab/scratch/mtong/datasets/seq/southard_RPE1_CRISPRi/southard_data\"\n",
    "result_id = \"sept_24_scGPT\"\n",
    "\n",
    "# create out_dir\n",
    "out_dir = f\"{working_dir}/results/scGPT\"\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 2\n",
    "\n",
    "n_hvg = 0  # number of highly variable genes\n",
    "include_zero_gene = \"all\"  # include zero expr genes in training input, \"all\", \"batch-wise\", \"row-wise\", or False\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "cell_emb_style = \"cls\"\n",
    "mvc_decoder_style = \"inner product, detach\"\n",
    "amp = True\n",
    "load_model = \"/.mounts/labs/steinlab/scratch/mtong/datasets/model/scGPT_pretrained\" # preloaded data can be downloaded from link in scGPT Github\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = epochs\n",
    "schedule_interval = 1\n",
    "early_stop = 5\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0.2  # dropout probability\n",
    "use_fast_transformer = True  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# loading data into PertData object\n",
    "pert_data_folder = Path(\"/.mounts/labs/steinlab/scratch/mtong/datasets/seq/southard_RPE1_CRISPRi\")\n",
    "pert_data = PertData(pert_data_folder)\n",
    "pert_data.load(data_path = f\"{working_dir}/{dataset_name}\")\n",
    "\n",
    "conds = pert_data.adata.obs[\"condition\"].cat.remove_unused_categories().cat.categories.tolist()\n",
    "gene_names = pert_data.adata.var[\"gene_name\"].values.tolist() + [\"ctrl\"]\n",
    "good_conds = np.array(conds)[[len(c) == 1 or (c[0] in gene_names and c[1] in gene_names) for c in [co.split(\"+\") for co in conds]]]\n",
    "pert_data.adata = pert_data.adata[[c in good_conds for c in pert_data.adata.obs[\"condition\"]],:]\n",
    "\n",
    "with open(test_train_config_path, \"r\") as json_file:\n",
    "    set2conditions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': ['RPL18A+ctrl', 'RPS8+ctrl', 'POGLUT3+ctrl', 'EIF3A+ctrl', 'RPL15+ctrl', 'POLR3E+ctrl', 'UXT+ctrl', 'PPP1R37+ctrl', 'RPL4+ctrl', 'KPNB1+ctrl', 'RPL38+ctrl', 'MED30+ctrl', 'RPS3A+ctrl', 'RPS15A+ctrl', 'TYK2+ctrl', 'RPS19+ctrl', 'EIF3CL+ctrl', 'ERCC3+ctrl', 'EXOSC8+ctrl', 'EIF1AX+ctrl', 'NUP214+ctrl', 'FUNDC2+ctrl', 'EIF3E+ctrl', 'RPTOR+ctrl', 'EXOSC5+ctrl', 'EIF3M+ctrl', 'RPS26+ctrl', 'GTF2E1+ctrl', 'TSR2+ctrl', 'MED1+ctrl', 'RPL35+ctrl', 'RPL23+ctrl', 'RPS4X+ctrl', 'RPL6+ctrl'], 'train': ['ctrl', 'ADAM10+ctrl', 'RPL14+ctrl', 'MED21+ctrl', 'MED11+ctrl', 'UTP18+ctrl', 'NUP205+ctrl', 'WDR43+ctrl', 'SIRT7+ctrl', 'MED18+ctrl', 'RPL37A+ctrl', 'NUP98+ctrl', 'EIF3I+ctrl', 'RPL32+ctrl', 'RPL39+ctrl', 'GLE1+ctrl', 'TMX2+ctrl', 'URI1+ctrl', 'RPS14+ctrl', 'XRCC5+ctrl', 'MED9+ctrl', 'RPS29+ctrl', 'RPS6+ctrl', 'TBCB+ctrl', 'POLR3D+ctrl', 'RPS13+ctrl', 'EXOSC3+ctrl', 'TAF5+ctrl', 'NUP85+ctrl', 'NUP88+ctrl', 'MED12+ctrl', 'RPS2+ctrl', 'RPLP0+ctrl', 'RPL30+ctrl', 'MED20+ctrl', 'EIF3D+ctrl', 'UBA52+ctrl', 'PIAS4+ctrl', 'RPL37+ctrl', 'EXOSC4+ctrl', 'NUP107+ctrl', 'NOL11+ctrl', 'RPL36+ctrl', 'BDP1+ctrl', 'PPP2CB+ctrl', 'MED14+ctrl', 'RPL31+ctrl', 'RPAP1+ctrl', 'XRCC6+ctrl', 'RPS3+ctrl', 'CSE1L+ctrl', 'SEH1L+ctrl', 'MED4+ctrl', 'MED8+ctrl', 'MED19+ctrl', 'NUP62+ctrl', 'RPL13+ctrl', 'POLR3F+ctrl', 'TANGO6+ctrl', 'RPL26+ctrl', 'RPL35A+ctrl', 'RPS18+ctrl', 'NKX6-1+ctrl', 'RPL17+ctrl', 'POLR2I+ctrl', 'MEIS3+ctrl', 'NUP160+ctrl', 'POLR3B+ctrl', 'TAF2+ctrl', 'TAF10+ctrl', 'RPL23A+ctrl', 'DDX47+ctrl', 'TAF6+ctrl', 'RPL8+ctrl', 'RPL9+ctrl', 'GPN2+ctrl', 'RPS23+ctrl', 'UTP15+ctrl', 'MTOR+ctrl', 'EIF3B+ctrl', 'FAU+ctrl', 'RPS24+ctrl', 'RPS9+ctrl', 'MED6+ctrl', 'RPL18+ctrl', 'RPL27A+ctrl', 'GTF2F2+ctrl', 'TAF8+ctrl', 'NUP133+ctrl', 'TAF3+ctrl'], 'val': ['DLD+ctrl', 'EXOSC7+ctrl', 'GTF2E2+ctrl', 'RPS7+ctrl', 'RPS27A+ctrl', 'ANKRD17+ctrl', 'NUP54+ctrl', 'RPS20+ctrl', 'RPL7+ctrl', 'EXOSC2+ctrl']}\n"
     ]
    }
   ],
   "source": [
    "# filter out problematic conditions\n",
    "set2conditions[\"train\"] = [c for c in set2conditions[\"train\"] if c in good_conds]\n",
    "set2conditions[\"test\"] = [c for c in set2conditions[\"test\"] if c in good_conds]\n",
    "set2conditions[\"val\"] = [c for c in set2conditions[\"val\"] if c in good_conds]\n",
    "\n",
    "print(set2conditions)\n",
    "pert_data.set2conditions = set2conditions\n",
    "pert_data.split = \"custom\"\n",
    "pert_data.subgroup = None\n",
    "pert_data.seed = 1\n",
    "pert_data.train_gene_set_size = 0.75\n",
    "pert_data.get_dataloader(batch_size = batch_size, test_batch_size = eval_batch_size)\n",
    "logger = scg.logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 3605/5129 genes in vocabulary of size 60697.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_dir = Path(load_model)\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "pert_data.adata.var[\"id_in_vocab\"] = [\n",
    "    1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
    "]\n",
    "gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
    "logger.info(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\"\n",
    ")\n",
    "genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "# not a lot of genes match here, might have to go back and convert gene names to standardized form to match pert_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loaded checkpoint /.mounts/labs/steinlab/scratch/mtong/datasets/model/scGPT_pretrained/best_model.pt with Wqkv conversion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerGenerator(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pert_encoder): Embedding(3, 512, padding_idx=2)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AffineExprDecoder(\n",
       "    (coeff_decoder): ExprDecoder(\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bias_decoder): ExprDecoder(\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modified code for model loading - does not require flash attention\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=False,  # no flash attention installed\n",
    ")\n",
    "\n",
    "if load_model is not None:\n",
    "    pretrained_dict = torch.load(model_file)\n",
    "\n",
    "    # Convert fused Wqkv weights to standard in_proj_weight/in_proj_bias\n",
    "    for layer_idx in range(nlayers):\n",
    "        wqkv_key = f'transformer_encoder.layers.{layer_idx}.self_attn.Wqkv.weight'\n",
    "        bqkv_key = f'transformer_encoder.layers.{layer_idx}.self_attn.Wqkv.bias'\n",
    "\n",
    "        if wqkv_key in pretrained_dict:\n",
    "            wqkv = pretrained_dict[wqkv_key]\n",
    "            bqkv = pretrained_dict[bqkv_key]\n",
    "\n",
    "            d = wqkv.shape[0] // 3  # hidden size\n",
    "\n",
    "            pretrained_dict[f'transformer_encoder.layers.{layer_idx}.self_attn.in_proj_weight'] = torch.cat(\n",
    "                [wqkv[:d, :], wqkv[d:2*d, :], wqkv[2*d:, :]], dim=0\n",
    "            )\n",
    "            pretrained_dict[f'transformer_encoder.layers.{layer_idx}.self_attn.in_proj_bias'] = torch.cat(\n",
    "                [bqkv[:d], bqkv[d:2*d], bqkv[2*d:]], dim=0\n",
    "            )\n",
    "\n",
    "            # remove fused keys\n",
    "            del pretrained_dict[wqkv_key]\n",
    "            del pretrained_dict[bqkv_key]\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    # Filter pretrained_dict to only include matching shapes\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    logger.info(f\"Loaded checkpoint {model_file} with Wqkv conversion.\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Resume model from /.mounts/labs/steinlab/scratch/mtong/datasets/model/scGPT_pretrained/best_model.pt, the model args will override the config /.mounts/labs/steinlab/scratch/mtong/datasets/model/scGPT_pretrained/args.json.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "logger.info(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: torch.utils.data.DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse = 0.0, 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    for batch, batch_data in enumerate(train_loader):\n",
    "        batch_size = len(batch_data.y)\n",
    "        batch_data.to(device)\n",
    "        x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
    "        ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
    "        pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
    "        target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
    "\n",
    "        if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "            if include_zero_gene == \"all\":\n",
    "                input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "            else:\n",
    "                input_gene_ids = (\n",
    "                    ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "                )\n",
    "            # sample input_gene_id\n",
    "            if len(input_gene_ids) > max_seq_len:\n",
    "                input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "                    :max_seq_len\n",
    "                ]\n",
    "            input_values = ori_gene_values[:, input_gene_ids]\n",
    "            input_pert_flags = pert_flags[:, input_gene_ids]\n",
    "            target_values = target_gene_values[:, input_gene_ids]\n",
    "\n",
    "            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "            mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
    "\n",
    "            # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "            src_key_padding_mask = torch.zeros_like(\n",
    "                input_values, dtype=torch.bool, device=device\n",
    "            )\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                mapped_input_gene_ids,\n",
    "                input_values,\n",
    "                input_pert_flags,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "            )\n",
    "            output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "            masked_positions = torch.ones_like(\n",
    "                input_values, dtype=torch.bool\n",
    "            )  # Use all\n",
    "            loss = loss_mse = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} |\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, val_loader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_data in enumerate(val_loader):\n",
    "            batch_size = len(batch_data.y)\n",
    "            batch_data.to(device)\n",
    "            x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
    "            ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
    "            pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
    "            target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
    "\n",
    "            if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "                if include_zero_gene == \"all\":\n",
    "                    input_gene_ids = torch.arange(n_genes, device=device)\n",
    "                else:  # when batch-wise\n",
    "                    input_gene_ids = (\n",
    "                        ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "                    )\n",
    "\n",
    "                # sample input_gene_id\n",
    "                if len(input_gene_ids) > max_seq_len:\n",
    "                    input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "                        :max_seq_len\n",
    "                    ]\n",
    "                input_values = ori_gene_values[:, input_gene_ids]\n",
    "                input_pert_flags = pert_flags[:, input_gene_ids]\n",
    "                target_values = target_gene_values[:, input_gene_ids]\n",
    "\n",
    "                mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "                mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
    "\n",
    "                # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "                src_key_padding_mask = torch.zeros_like(\n",
    "                    input_values, dtype=torch.bool, device=input_values.device\n",
    "                )\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                    mapped_input_gene_ids,\n",
    "                    input_values,\n",
    "                    input_pert_flags,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    CLS=CLS,\n",
    "                    CCE=CCE,\n",
    "                    MVC=MVC,\n",
    "                    ECS=ECS,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = torch.ones_like(\n",
    "                    input_values, dtype=torch.bool, device=input_values.device\n",
    "                )\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "            total_loss += loss.item()\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item()\n",
    "    return total_loss / len(val_loader), total_error / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction function - mean values\n",
    "def predict(\n",
    "    model: TransformerGenerator, pert_list, pool_size = None\n",
    "):\n",
    "    adata = pert_data.adata\n",
    "    ctrl_adata = adata[adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    if pool_size is None:\n",
    "        pool_size = len(ctrl_adata.obs)\n",
    "    gene_list = pert_data.gene_names.values.tolist()\n",
    "    for pert in pert_list:\n",
    "        for i in pert:\n",
    "            if i not in gene_list:\n",
    "                raise ValueError(\n",
    "                    \"The gene is not in the perturbation graph. Please select from GEARS.gene_list!\"\n",
    "                )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        results_pred = {}\n",
    "        for pert in pert_list:\n",
    "            cell_graphs = create_cell_graph_dataset_for_prediction(\n",
    "                pert, ctrl_adata, gene_list, device, num_samples=pool_size\n",
    "            )\n",
    "            loader = DataLoader(cell_graphs, batch_size=eval_batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch_data in loader:\n",
    "                pred_gene_values = model.pred_perturb(\n",
    "                    batch_data, include_zero_gene, gene_ids=gene_ids, amp=amp\n",
    "                )\n",
    "                preds.append(pred_gene_values)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            results_pred[\"_\".join(pert)] = np.mean(preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return results_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = copy.deepcopy(model)\n",
    "patience = patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   0 | 100/819 batches | lr 0.0001 | ms/batch 709.35 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   0 | 200/819 batches | lr 0.0001 | ms/batch 380.02 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 300/819 batches | lr 0.0001 | ms/batch 380.17 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 400/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 500/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 600/819 batches | lr 0.0001 | ms/batch 380.17 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 700/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   0 | 800/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   0 | time: 354.84s | valid loss/mse 0.0924 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.0924\n",
      "scGPT - INFO - | epoch   1 | 100/819 batches | lr 0.0001 | ms/batch 384.17 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 200/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 300/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 400/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 500/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 600/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 700/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 800/819 batches | lr 0.0001 | ms/batch 379.96 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 316.81s | valid loss/mse 0.0937 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   2 | 100/819 batches | lr 0.0001 | ms/batch 383.82 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 200/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   2 | 300/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 400/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   2 | 500/819 batches | lr 0.0001 | ms/batch 380.34 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 600/819 batches | lr 0.0001 | ms/batch 380.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 700/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 800/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 316.90s | valid loss/mse 0.0960 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   3 | 100/819 batches | lr 0.0001 | ms/batch 383.84 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   3 | 200/819 batches | lr 0.0001 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 300/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 400/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 500/819 batches | lr 0.0001 | ms/batch 379.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 600/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 700/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 800/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 316.78s | valid loss/mse 0.0928 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   4 | 100/819 batches | lr 0.0001 | ms/batch 383.84 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   4 | 200/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 300/819 batches | lr 0.0001 | ms/batch 379.96 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   4 | 400/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 500/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 600/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 700/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 800/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 316.80s | valid loss/mse 0.0922 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.0922\n",
      "scGPT - INFO - | epoch   5 | 100/819 batches | lr 0.0001 | ms/batch 383.93 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   5 | 200/819 batches | lr 0.0001 | ms/batch 380.16 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   5 | 300/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 400/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   5 | 500/819 batches | lr 0.0001 | ms/batch 380.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 600/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 700/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 800/819 batches | lr 0.0001 | ms/batch 380.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 316.85s | valid loss/mse 0.0929 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   6 | 100/819 batches | lr 0.0001 | ms/batch 383.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 200/819 batches | lr 0.0001 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 300/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 400/819 batches | lr 0.0001 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 500/819 batches | lr 0.0001 | ms/batch 380.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 600/819 batches | lr 0.0001 | ms/batch 380.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 700/819 batches | lr 0.0001 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 800/819 batches | lr 0.0001 | ms/batch 379.99 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 316.84s | valid loss/mse 0.0923 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   7 | 100/819 batches | lr 0.0000 | ms/batch 384.02 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   7 | 200/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 300/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 400/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 500/819 batches | lr 0.0000 | ms/batch 380.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 600/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 700/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 800/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 316.94s | valid loss/mse 0.0925 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   8 | 100/819 batches | lr 0.0000 | ms/batch 384.42 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 200/819 batches | lr 0.0000 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 300/819 batches | lr 0.0000 | ms/batch 380.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 400/819 batches | lr 0.0000 | ms/batch 380.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 500/819 batches | lr 0.0000 | ms/batch 379.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 600/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 700/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   8 | 800/819 batches | lr 0.0000 | ms/batch 381.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 317.02s | valid loss/mse 0.0935 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   9 | 100/819 batches | lr 0.0000 | ms/batch 384.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 200/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 300/819 batches | lr 0.0000 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 400/819 batches | lr 0.0000 | ms/batch 380.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 500/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 600/819 batches | lr 0.0000 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 700/819 batches | lr 0.0000 | ms/batch 380.34 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   9 | 800/819 batches | lr 0.0000 | ms/batch 380.04 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 316.94s | valid loss/mse 0.0921 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.0921\n",
      "scGPT - INFO - | epoch  10 | 100/819 batches | lr 0.0000 | ms/batch 384.68 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch  10 | 200/819 batches | lr 0.0000 | ms/batch 380.95 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  10 | 300/819 batches | lr 0.0000 | ms/batch 380.80 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch  10 | 400/819 batches | lr 0.0000 | ms/batch 380.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  10 | 500/819 batches | lr 0.0000 | ms/batch 380.59 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  10 | 600/819 batches | lr 0.0000 | ms/batch 380.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  10 | 700/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  10 | 800/819 batches | lr 0.0000 | ms/batch 380.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 317.24s | valid loss/mse 0.0920 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.0920\n",
      "scGPT - INFO - | epoch  11 | 100/819 batches | lr 0.0000 | ms/batch 384.40 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 200/819 batches | lr 0.0000 | ms/batch 380.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 300/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 400/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 500/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 600/819 batches | lr 0.0000 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 700/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  11 | 800/819 batches | lr 0.0000 | ms/batch 380.14 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 316.94s | valid loss/mse 0.0912 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.0912\n",
      "scGPT - INFO - | epoch  12 | 100/819 batches | lr 0.0000 | ms/batch 384.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 200/819 batches | lr 0.0000 | ms/batch 380.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 300/819 batches | lr 0.0000 | ms/batch 380.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 400/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 500/819 batches | lr 0.0000 | ms/batch 380.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 600/819 batches | lr 0.0000 | ms/batch 380.20 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 700/819 batches | lr 0.0000 | ms/batch 380.40 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  12 | 800/819 batches | lr 0.0000 | ms/batch 380.36 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 317.00s | valid loss/mse 0.0929 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  13 | 100/819 batches | lr 0.0000 | ms/batch 384.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 200/819 batches | lr 0.0000 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 300/819 batches | lr 0.0000 | ms/batch 380.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 400/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 500/819 batches | lr 0.0000 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 600/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 700/819 batches | lr 0.0000 | ms/batch 379.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  13 | 800/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 316.86s | valid loss/mse 0.0918 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch  14 | 100/819 batches | lr 0.0000 | ms/batch 384.84 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 200/819 batches | lr 0.0000 | ms/batch 379.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 300/819 batches | lr 0.0000 | ms/batch 379.99 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch  14 | 400/819 batches | lr 0.0000 | ms/batch 380.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 500/819 batches | lr 0.0000 | ms/batch 380.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 600/819 batches | lr 0.0000 | ms/batch 380.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 700/819 batches | lr 0.0000 | ms/batch 380.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch  14 | 800/819 batches | lr 0.0000 | ms/batch 380.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 317.02s | valid loss/mse 0.0917 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model training/fine-tuning\n",
    "for epoch in range(0, epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "    )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} |\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models and generate predictions\n",
    "conds = pert_data.adata.obs[\"condition\"].cat.remove_unused_categories().cat.categories.tolist()\n",
    "split_conds = [x.split(\"+\") for x in conds]\n",
    "split_conds = [list(filter(lambda y: y != \"ctrl\", x)) for x in split_conds]\n",
    "\n",
    "out_dir_model = \"/.mounts/labs/steinlab/scratch/mtong/datasets/seq/southard_RPE1_CRISPRi/southard_data/results/sept_24_scGPT\"\n",
    "torch.save(best_model.state_dict(), f\"{out_dir_model}/full_best_model.pt\")\n",
    "\n",
    "all_pred_vals = predict(best_model, split_conds, pool_size=pool_size)\n",
    "all_pred_vals = {k: v.tolist() for k, v in all_pred_vals.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ground truth\n",
    "ground_truth_vals = {}\n",
    "for cond in conds:\n",
    "    obs_idx = pert_data.adata.obs['condition'] == cond\n",
    "    mean_expr = np.asarray(pert_data.adata[obs_idx, :].X.mean(axis=0)).ravel()\n",
    "    # Remove \"+ctrl\" from the key to match prediction\n",
    "    key = cond.replace(\"+ctrl\", \"\")\n",
    "    ground_truth_vals[key] = mean_expr.tolist()\n",
    "\n",
    "# convert empty string key to 'ctrl'\n",
    "if '' in all_pred_vals:\n",
    "    all_pred_vals['ctrl'] = all_pred_vals.pop('')\n",
    "\n",
    "# sanity check\n",
    "pred_conditions = set(all_pred_vals.keys())\n",
    "gt_conditions = set(ground_truth_vals.keys())\n",
    "assert pred_conditions == gt_conditions, f\"Mismatch in conditions: {pred_conditions ^ gt_conditions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python done\n"
     ]
    }
   ],
   "source": [
    "# save \n",
    "with open(f\"{out_dir}/all_predictions.json\", 'w', encoding=\"utf8\") as handle:\n",
    "    json.dump(all_pred_vals, handle, indent = 4)\n",
    "with open(f\"{out_dir}/gene_names.json\", 'w', encoding=\"utf8\") as handle:\n",
    "    json.dump(pert_data.adata.var[\"gene_name\"].values.tolist(), handle, indent = 4)\n",
    "with open(f\"{out_dir}/all_ground_truth.json\", 'w', encoding=\"utf8\") as handle:\n",
    "    json.dump(ground_truth_vals, handle, indent=4)\n",
    "\n",
    "session_info.show()\n",
    "print(\"Python done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
